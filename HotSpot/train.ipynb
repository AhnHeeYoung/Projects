{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 16:19:30,738 - INFO - Model Load\n",
      "2021-08-11 16:19:30,738 - INFO - Model Load\n",
      "2021-08-11 16:19:30,744 - INFO -  Patch Size : 2048 \n",
      "2021-08-11 16:19:30,744 - INFO -  Patch Size : 2048 \n",
      "2021-08-11 16:19:30,744 - INFO - \n",
      "\n",
      "2021-08-11 16:19:30,744 - INFO - \n",
      "\n",
      "2021-08-11 16:19:30,745 - INFO - Epoch : 1\n",
      "2021-08-11 16:19:30,745 - INFO - Epoch : 1\n",
      "2021-08-11 16:19:30,746 - INFO - Train Patient : 310\n",
      "2021-08-11 16:19:30,746 - INFO - Train Patient : 310\n",
      "2021-08-11 16:19:30,746 - INFO - Test Patient : 78\n",
      "2021-08-11 16:19:30,746 - INFO - Test Patient : 78\n",
      "2021-08-11 16:19:30,747 - INFO - \n",
      "\n",
      "2021-08-11 16:19:30,747 - INFO - \n",
      "\n",
      "2021-08-11 16:19:30,752 - INFO - Train -> 0 : 11502개, 1 : 11502개\n",
      "2021-08-11 16:19:30,752 - INFO - Train -> 0 : 11502개, 1 : 11502개\n",
      "2021-08-11 16:19:30,753 - INFO - Test ->  0 : 2283개, 1 : 2283개\n",
      "2021-08-11 16:19:30,753 - INFO - Test ->  0 : 2283개, 1 : 2283개\n",
      "2021-08-11 16:19:30,754 - INFO - \n",
      "\n",
      "2021-08-11 16:19:30,754 - INFO - \n",
      "\n",
      "2021-08-11 16:20:07,535 - INFO - Train Epoch : 1/1000, Batch : 1/1438, ACC : 0.562, AUC : 0.578, Recall : 0.625, Precision : 0.556, Loss : 0.716\n",
      "2021-08-11 16:20:07,535 - INFO - Train Epoch : 1/1000, Batch : 1/1438, ACC : 0.562, AUC : 0.578, Recall : 0.625, Precision : 0.556, Loss : 0.716\n",
      "2021-08-11 16:20:07,538 - INFO - TN : 4 FP : 4\n",
      "2021-08-11 16:20:07,538 - INFO - TN : 4 FP : 4\n",
      "2021-08-11 16:20:07,538 - INFO - FN : 3 TP : 5\n",
      "2021-08-11 16:20:07,538 - INFO - FN : 3 TP : 5\n",
      "2021-08-11 16:20:07,539 - INFO - \n",
      "\n",
      "2021-08-11 16:20:07,539 - INFO - \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1252c3fb7f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xml.etree.ElementTree import parse\n",
    "import numpy as np\n",
    "#from Class_ID_Name import *\n",
    "import os\n",
    "import openslide\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "\n",
    "import cv2 as cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from skimage import draw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "patch_size = 2048\n",
    "\n",
    "all_patient_name = [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-1/*.pickle'.format(patch_size)))] + \\\n",
    "                   [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-2/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-3/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-4/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-5/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-6/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-7/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-8/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-9/*.pickle'.format(patch_size)))] + \\\n",
    "                    [i.split('/')[-1].split('.')[0] for i in sorted(glob.glob('Dict{}/Page-10/*.pickle'.format(patch_size)))]\n",
    "\n",
    "\n",
    "\n",
    "dict_path = [i for i in sorted(glob.glob('Dict{}/Page-1/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-2/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-3/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-4/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-5/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-6/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-7/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-8/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-9/*.pickle'.format(patch_size)))] + \\\n",
    "            [i for i in sorted(glob.glob('Dict{}/Page-10/*.pickle'.format(patch_size)))]\n",
    "    \n",
    "All_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "for path in dict_path:\n",
    "\n",
    "    with open('{}'.format(path), 'rb') as ok:\n",
    "        All_dict.update(pickle.load(ok))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class HotspotDataset(Dataset):\n",
    "\n",
    "    def __init__(self, patient_name, All_dict, mode, patch_size):\n",
    "        \n",
    "        self.patient_name = patient_name\n",
    "        self.All_dict = All_dict\n",
    "        self.mode = mode\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        \n",
    "        self.number_coords = 0\n",
    "        for name in self.patient_name:\n",
    "            self.number_coords += len(self.All_dict[name]['coords'])\n",
    "\n",
    "            \n",
    "        self.label = []\n",
    "        for name in self.patient_name:\n",
    "            self.label += self.All_dict[name]['label']              \n",
    "            \n",
    "            \n",
    "        \n",
    "        self.all_coords = []\n",
    "        self.all_labels = []\n",
    "        self.all_slides_path = []\n",
    "        self.all_patient_name = []\n",
    "        \n",
    "        for name in self.patient_name:\n",
    "            self.all_coords += self.All_dict[name]['coords']\n",
    "            self.all_labels += self.All_dict[name]['label']\n",
    "\n",
    "            for i in range(len(self.All_dict[name]['coords'])):\n",
    "                self.all_slides_path += [self.All_dict[name]['slides_path']]\n",
    "                self.all_patient_name += [name]      \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_coords\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        \n",
    "        slide_path = self.all_slides_path[idx]\n",
    "        \n",
    "        image = openslide.OpenSlide(slide_path)\n",
    "        image = np.array(image.read_region((self.all_coords[idx][0], self.all_coords[idx][1]), 0, (self.patch_size, self.patch_size)))[:, :, :3]\n",
    "        image = cv2.resize(image, None, fx=0.25, fy=0.25)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            output = A.Compose([\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.VerticalFlip(p=0.5),\n",
    "                        #A.Cutout(num_holes=4, max_h_size=40, max_w_size=40, p=0.5),\n",
    "                        A.ColorJitter(brightness=0.2, contrast=0.5, saturation=0.1),\n",
    "                        A.RandomRotate90(p=0.5)])(image=image)\n",
    "            image = output['image']    \n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        \n",
    "        \n",
    "        label = self.all_labels[idx]\n",
    "        coords = [self.all_coords[idx][0], self.all_coords[idx][1]]\n",
    "        patient_name = self.all_patient_name[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\"image\" : image, \"label\" : label, \"patient_name\" : patient_name, \"coords\" : coords, \"slide_path \" : slide_path}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######### Shuffle ########\n",
    "all_patient_name = np.array(all_patient_name)\n",
    "np.random.shuffle(all_patient_name)\n",
    "all_patient_name = all_patient_name.tolist()\n",
    "######### Shuffle ########\n",
    "\n",
    "\n",
    "    \n",
    "len_ = len(all_patient_name)\n",
    "idx = int(len_*0.8)\n",
    "    \n",
    "train_patient = all_patient_name[:idx]\n",
    "val_patient = all_patient_name[idx:]\n",
    "\n",
    "    \n",
    "train_dataset = HotspotDataset(train_patient, All_dict, mode='train', patch_size=patch_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, num_workers=12, shuffle=True)\n",
    "\n",
    "val_dataset = HotspotDataset(val_patient, All_dict, mode='test', patch_size=patch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, num_workers=12, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "run_info = 'patch_size_{}'.format(patch_size)\n",
    "\n",
    "if not os.path.exists('checkpoints/{}'.format(run_info)):\n",
    "    os.mkdir('checkpoints/{}'.format(run_info))\n",
    "\n",
    "log = logging.getLogger('staining_log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fileHandler = logging.FileHandler('checkpoints/{}/log.txt'.format(run_info))\n",
    "streamHandler = logging.StreamHandler()\n",
    "fileHandler.setFormatter(formatter)\n",
    "streamHandler.setFormatter(formatter)\n",
    "#\n",
    "log.addHandler(fileHandler)\n",
    "log.addHandler(streamHandler)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, 2, bias=True)\n",
    "model.to(device)\n",
    "log.info(\"Model Load\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "#scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50, eta_min=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "import torch.cuda.amp as amp  \n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "num_epochs=1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #scheduler_cosine.step()\n",
    "    log.info(' Patch Size : {} '.format(patch_size))\n",
    "    log.info('\\n')\n",
    "    \n",
    "    log.info(\"Epoch : {}\".format(epoch+1))\n",
    "    \n",
    "    log.info('Train Patient : {}'.format(len(train_patient)))\n",
    "    log.info('Test Patient : {}'.format(len(val_patient)))\n",
    "    log.info('\\n')\n",
    "    \n",
    "    log.info(\"Train -> 0 : {}개, 1 : {}개\".format(np.bincount(np.array(train_dataset.label))[0], np.bincount(np.array(train_dataset.label))[1]))    \n",
    "    log.info(\"Test ->  0 : {}개, 1 : {}개\".format(np.bincount(np.array(val_dataset.label))[0], np.bincount(np.array(val_dataset.label))[1]))\n",
    "    log.info('\\n')\n",
    "    \n",
    "    train_loss_sum = []\n",
    "    train_pred_label = []\n",
    "    train_true_label = []\n",
    "    train_pred = []\n",
    "    \n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        image = batch['image'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        #with amp.autocast():\n",
    "        \n",
    "        output = model(image)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #scaler.scale(loss).backward()\n",
    "        #scaler.step(optimizer)\n",
    "        #scaler.update()\n",
    "\n",
    "        train_loss_sum += [loss.detach().cpu().tolist()]\n",
    "        train_true_label += label.cpu().detach().numpy().tolist()\n",
    "        train_pred_label += np.argmax(nn.Softmax(1)(output.cpu().detach()), 1)\n",
    "        \n",
    "        \n",
    "        pred_prob = nn.Softmax(1)(output.cpu().detach()).numpy()\n",
    "        train_pred += pred_prob[:, 1].tolist()\n",
    "        \n",
    "\n",
    "        true_number = np.array(train_true_label) == np.array(train_pred_label)\n",
    "        train_loss = sum(train_loss_sum) / len(train_loss_sum)\n",
    "        Accuracy = sum(true_number) / len(true_number)\n",
    "\n",
    "        fpr, tpr, threshold = roc_curve(np.array(train_true_label), np.array(train_pred))\n",
    "        AUC = auc(fpr, tpr)\n",
    "\n",
    "        conf_mat = confusion_matrix(y_true = np.array(train_true_label), y_pred = np.array(train_pred_label))\n",
    "        \n",
    "        TN = conf_mat[0, 0]\n",
    "        FP = conf_mat[0, 1]\n",
    "        FN = conf_mat[1, 0]\n",
    "        TP = conf_mat[1, 1]\n",
    "        \n",
    "        Recall = TP / (TP + FN)\n",
    "        Precision = TP / (TP + FP)\n",
    "        \n",
    "        if (idx+1) % 1 == 0:\n",
    "            log.info(\"Train Epoch : {}/{}, Batch : {}/{}, ACC : {:.3f}, AUC : {:.3f}, Recall : {:.3f}, Precision : {:.3f}, Loss : {:.3f}\"\n",
    "                     .format(epoch+1, num_epochs, idx+1, len(train_dataloader), Accuracy, AUC, Recall, Precision, train_loss))\n",
    "            \n",
    "            log.info(\"TN : {} FP : {}\".format(TN, FP))\n",
    "            log.info(\"FN : {} TP : {}\".format(FN, TP))\n",
    "            log.info(\"\\n\")\n",
    "\n",
    "    log.info(\"Train Epoch : {}/{}, Batch : {}/{}, ACC : {:.3f}, AUC : {:.3f}, Recall : {:.3f}, Precision : {:.3f}, Loss : {:.3f}\"\n",
    "             .format(epoch+1, num_epochs, idx+1, len(train_dataloader), Accuracy, AUC, Recall, Precision, train_loss))\n",
    "    \n",
    "    log.info(\"0 : {}개, 1 : {}개\".format(np.bincount(np.array(train_dataset.label))[0], np.bincount(np.array(train_dataset.label))[1]))\n",
    "    log.info(\"TN : {} FP : {}\".format(TN, FP))\n",
    "    log.info(\"FN : {} TP : {}\".format(FN, TP))\n",
    "    \n",
    "    log.info(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_label[idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
